---
layout: default
title: "Forecasting GPT-4"
permalink: /forecasting-gpt4
author: Thomas
---

# [DRAFT] Forecasting GPT-4


<p align="center">
  <img src="/images/binoculars_camera_colored_pencil.png" width="40%" />
</p>

ML researchers joke that each month brings new surprises. I’d like to be less surprised in the coming month, so I’ve tried to forecast what GPT-4 will bring (or PaLM-2, Meerkat, …).

Three years after GPT-3 only three companies have the secret sauce for large language models (LLMs) and write about them: OpenAI, DeepMind, and Google. I expect the next big model to be from one of the trio - most obviously OpenAI, which conspicuously missed its spring / summer GPT announcement this year.

**Interesting predictions**:
- 16k-32k context window
- Tool use
- Improved user interaction
- Incorporates user-generated data (if OpenAI)
- Improved scaling (if not DeepMind)

**Boring predictions (I only cover some of these)**:
- Between 100-500B parameters
- Encouraging Intermediate generation
- Either trained on task data or with human feedback (e.g. RLHF). 
- Chinchilla scaling laws or better
- Many pretraining data improvements
- Improved decoding techniques, hyperparameter search, tokenization
- Not multimodal (if OpenAI)


<p align="center">
  <img src="/images/OpenAI_model_timeline.png" alt="timeline of selected OpenAI model announcements" />
  <div align="center" class="caption">OpenAI has yet to release GPT-4 this year.</div>
</p>
<br>
<p align="center">
  <img src="/images/DeepMind_model_timeline.png"/>
</p>
<br>
<p align="center">
  <img src="/images/Google_model_timeline.png"/>
</p>

## Interesting predictions
- **Longer context.** LLMs in production need to handle much longer inputs than the toy experiments in Playground, for reasoning over longer texts, using very long prompts, or including many in context examples. GPT-2 (2019) has a context window of 2^9 = 512 tokens. GPT-3 (2020) has a 4x larger window at 2^11 = 2048 tokens. Anthropic’s LLMs described in 2021 have a 4x larger window than GPT-3 at 2^13 = 8192 tokens, or about 6000 words. Noting the year-on-year fourfold increase, I expect the next big model in 2022 to have a context window of 2^15 = 32,768 tokens. This is a nontrivial engineering challenge, so I’ll hedge by saying it will be either 2^14 or 2^15.
    - Did you know that `text-davinci-002` already does 4k tokens? It's not documented anywhere, but
      you can see for yourself:

<p align="center">
  <img src="/images/davinci-002-4k.png"/>
</p>

- **Tool use**. OpenAI, DeepMind, and Google have all released models in the past year trained to browse the internet, giving AI unfettered access to the world’s knowledge. Web browsing means that models don’t have to be retrained to teach them more information; they can simply Google facts they need. More generally, tool use helps models output correct answers and perform more specialized computations. Sufficiently large models already have latent tool use abilities: GPT-3 [can be prompted](https://twitter.com/sergeykarayev/status/1569377881440276481) to write Python code to answer math questions or even write API requests (!) to query for information it doesn’t know. I expect general model releases to start including tool use capabilities. I predict the next GPT-4 will improve on WebGPT capabilities, but these tool use capabilities will be heavily restricted to avoid e.g. inadvertently sending malicious API requests. 
- **Incorporating user-generated data** (if OpenAI). The [InstructGPT paper](https://arxiv.org/abs/2203.02155) uses prompts generated by users in Playground (but not using the API in production). The appeal of user-generated data is the sheer diversity, since you have far more people writing prompts than you could have through managing your own labellers. However, Google, DeepMind, and FB do not offer commercial LLM APIs, so they have no way to obtain this data. AI21, OpenAI, and Cohere do. A meaningful forecast here would predict the change in quantity of user-generated data used. But since OpenAI didn't disclose this information, it's impossible to make an exact prediction.
- **Improved scaling**. DeepMind’s [Chinchilla work](https://arxiv.org/abs/2203.15556) improved on earlier scaling laws. It’s likely that there is still some room for improved scaling behaviour. I predict the next big model released by Google or OpenAI will follow a different scaling law than Chinchilla.
  - Note that PaLM has a much steeper slope than other models for task performance over parameter count on certain tasks. I don't know why.

<p align="center">
  <img src="/images/conceptual_combinations.png" width="500"/>
</p>

## Boring predictions
- **Not that large**. I predict that the largest size for OpenAI and DeepMind’s next models to be between 50B to 500B parameters. Especially given DeepMind's recent Chinchilla work, I would be extremely surprised if their next models were 1T parameters. My impression is that Google is most likely of the three to train a dense 1T model first.
- **Pretraining data improvements**. The most extreme scaling bulls would have you believe that dumping “whole mountains of barely filtered, mostly unedited scrapes of the internet into the eager maw of a hungry model” CITE is the way of the future. I’m confident there will be increasing attention to the pretraining data over the next 18 months, primarily focusing on processing data carefully and curating data sources. The amount of data will remain the dominant factor for performance (over quality), but companies will pluck low hanging fruit here. Note that for multimodal models, reducing noise in data is critical and matters more than scale: no amount of captions help multimodality if the captions do not describe what’s in the image.
  - **Pretraining data improvements**. The most extreme scaling bulls would have you believe that dumping “whole mountains of barely filtered, mostly unedited scrapes of the internet into the eager maw of a hungry model” CITE is the way of the future. I’m confident there will be increasing attention to the pretraining data over the next 18 months, primarily focusing on processing data carefully and curating data sources. The amount of data will remain the dominant factor for performance (over quality), but companies will pluck low hanging fruit here. Note that for multimodal models, reducing noise in data is critical and matters more than scale: no amount of captions help multimodality if the captions do not describe what’s in the image.
  - **Increasing specialized data sources**. While GPT-2 was trained primarily on WebText, a dataset scraped from outbound reddit links, GPT-3 included a (noisier) web scrape, Common Crawl, as well as two books corpora (Books1, Books2) and en-Wikipedia. The general trend has been to add more specialized data sources, for example the Pile (OPT, GLM-130B, etc) includes arXiv, PubMed, FreeLaw, StackExchange, USPTO, etc. I would expect the next big model to ingest even more PDFs and specialized documents from the internet (or focus on cleaning up those subsets of common crawl).
    - It’s interesting to compare GPT-2 vs GPT-3 at similar parameter counts (1.5 vs 1.3B). On WMT ‘14 Fr->En, GPT-2 1.5B gets 5 BLEU. GPT-3 1.3B gets about 22 BLEU. On CoQA, GPT 2 1.5B: 55 F1; GPT-3 1.3B: 65 F1. On Lambada, GPT2 1.5B: 63%, GPT3 1.3B: 56% (there is a weird drop in the curve here, otherwise it would be around 72%). (Note that amount of compute is different)
  - **Improving dataset filtering**. Work since GPT-3 (2020) usually filter the web scrape part of their dataset with e.g. a quality classifier trained on a higher quality dataset such as WebText. (WebText was built by scraping outbound links from Reddit as a proxy for quality). This is a crude approach which is likely to filter out mangled or empty documents, but may resemble document similarity for OOD high quality documents; in other words, a WebText classifier is looking for documents that redditors like and not just quality (my speculation). I think training a classifier on a more varied “high quality documents” dataset is a simple improvement. There should be even better automated ways to automatically filter for high quality documents. However I think it’s unlikely the next big model will improve significantly in this regard, because ML engineers don’t like to think about data and because it’s expensive to measure how improving data quality improves model performance (need to train a model). Also, I think most easy improvements here will get washed away by scale.
  - **On preserving data structure**. Structured data like tables CITE seem to improve in-context learning. The intuition is that each row or example functions like few-shot prompt examples, so the structure is similar to in-context learning. My understanding is that web scrapes often mangle table structure, so it’s possible that the next big model will specially process this kind of structured data. I think this starts to get too into the weeds and is likely not worth doing as part of the pretraining data pipeline.
  - **On data source diversity**. It’s hard to improve over web crawls for data source diversity per se, unless you start digitizing physical media (like scanning books), converting other digital sources (like video captions), or accessing social media, which runs serious privacy risks, so I don’t expect a concerted effort on this front. 
  - **On synthetic data**. I do not expect any inclusion of synthetic data in the pretraining data, even for code or math datasets. (A) the research in this area is still nascent, and we don’t know what makes for good synthetic data (B) there is sufficient data from natural sources, so synthetic data is not yet needed.
- **Improved decoding techniques, hyperparameter search, tokenization**. These are all relatively small improvements, so they may not be included. Improving tokenization (e.g. tokenizing all digits separately to improve math performance) might be a big deal.

## Challenges in predicting performance.
The world’s best (disclosed) LLM is locked away by Google. Neither has anyone announced a model combining well-known separate improvements. The uncertainty around how the best or best possible models today behave ripples into the future. The next best model jumps off from a model we don’t have access to. Do iterative techniques combine, brick by brick, to build higher? Or does each new trick substitute for another?

Major labs withhold results until convenient, delaying announcements by half a year or more. Papers emerge during conference season and hibernate in between. It’s hard to gauge whether enough time has passed for new advancements if you don’t know when to start timing.

## Endnotes

Citations above are not comprehensive. If an important paper should be included, feel free to tweet it [@thomasiliao](twitter.com/thomasiliao).

Thanks to [Charlie Snell](twitter.com/sea_snell) for comments.

